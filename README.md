# Mini Project 
- *Selenium Libaray , Firebase  Authentication, PYMongo , PYQT5*


# Overview :
I have created a project in which I have used Selenium library  to scrape data also using Firebase Authentication
 for Sign in & Log in, PYMongo for storing scrape data , and PYQT5.

# Features 
- Using pyqt5 to for better experience.
- Using Firebase Authentication for sign in & log in mail should be used as a user unique id. 
- Using PYMongo for store product data.
- Using Selenium library for scraping.

# How to use :
- User need to Sign in or log in to account.
- If the user not sign in or log in the data not scrape properly.
- After sign in or log in the Daraz scraper dashboard open now add product name & db_collection name that you want to scrape data.
- After add product name & db_collection name click on button Scrape data it takes 1 to 3 mints to scrape product data after that it also show that scraping is complete.
- If you want to view scrape data then click on view scrape data then new window appear that show scrape data.

 : Here is demo 

1_ You can see when user run main.py then Firebase Authentication windows show. Sign in or Log in it depends on you. If you already sign in then you can log in easily.


2_ Now you can add your mail and password. Remember this your mail is used as your unique id when you scrape and view data.


3_ Now Daraz Dashboard windows open you can add product name & db_collection name to scrape data, click on Scrape data button, after scraping it also show that scraping is complete.


4_ If you want to view scrape data then click on view data scrape button.

# Requirements
- Python Version 3.12.9
- Selenium Libaray 
- MongoDBCompass
- PYQT5



